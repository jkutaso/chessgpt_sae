{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chessgpt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import einops\n",
    "import wandb\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import random\n",
    "from IPython.display import display\n",
    "import wandb\n",
    "from jaxtyping import Float, Int, Bool, Shaped, jaxtyped\n",
    "from typing import Union, Optional, Tuple, Callable, Dict\n",
    "from collections import Counter\n",
    "import typeguard\n",
    "from functools import partial\n",
    "import copy\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import HookedRootModule, HookPoint\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from rich import print as rprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import circuits.eval_sae_as_classifier as eval_sae\n",
    "import circuits.analysis as analysis\n",
    "import circuits.eval_board_reconstruction as eval_board_reconstruction\n",
    "import circuits.get_eval_results as get_eval_results\n",
    "import circuits.f1_analysis as f1_analysis\n",
    "import circuits.utils as utils\n",
    "import circuits.pipeline_config as pipeline_config\n",
    "from circuits.dictionary_learning.dictionary import AutoEncoder, GatedAutoEncoder, AutoEncoderNew\n",
    "import common\n",
    "import chess_utils\n",
    "import chess\n",
    "#from plotly_utils import imshow\n",
    "#from neel_plotly import scatter, line\n",
    "\n",
    "\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "import pickle\n",
    "with open('meta.pkl', 'rb') as picklefile:\n",
    "    meta = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chessgpt/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading statistics aggregation dataset\n"
     ]
    }
   ],
   "source": [
    "autoencoder = common.load_autoencoder(device)\n",
    "model = common.load_model(device)\n",
    "dataset = common.get_dataset(device)\n",
    "TRAIN_TEST_GAME_SPLIT = 2500\n",
    "rows_to_include = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs_tensor = t.stack([t.tensor(x) for x in dataset['encoded_inputs']]).to(device)\n",
    "is_check = dataset['board_to_check_state'].squeeze(-1)\n",
    "is_this_a_dot = (encoded_inputs_tensor == meta['stoi']['.']).to(device)\n",
    "is_this_a_dot = is_this_a_dot[:rows_to_include]\n",
    "is_this_a_dot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_results = common.get_aggregation_results(1000)\n",
    "formatted_results = common.get_formatted_results(aggregation_results)\n",
    "features_for_check_state = common.get_true_feature_indices(formatted_results, \"board_to_check_state\")\n",
    "features_for_check_state = features_for_check_state.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "#Collect all the activations for the features we care about\n",
    "relevant_sae_activations = []\n",
    "def get_activation(name, relevant_features):\n",
    "    def hook(model, input, output):\n",
    "        encoded_activations = autoencoder.encode(output[0]) # batch_size x len_seq x n_features_sae\n",
    "        relevant_sae_activations.append(encoded_activations[:, :, relevant_features])\n",
    "    return hook\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "activation_handle = model.transformer.h[5].register_forward_hook(get_activation(f\"resid_stream_{5}\", np.arange(4096)))\n",
    "num_batches = 50\n",
    "batch_size = rows_to_include // num_batches\n",
    "if device == 'cpu':\n",
    "    num_batches = 1\n",
    "    batch_size = 1\n",
    "for i in range(num_batches):\n",
    "    print(i)\n",
    "    model(encoded_inputs_tensor[i*batch_size:(i+1)*batch_size])\n",
    "    t.cuda.empty_cache()\n",
    "activation_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([250, 256, 4096]), torch.Size([250, 256]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_relevant_sae_activations = t.concat(relevant_sae_activations, dim=0) # batch_size x len_seq x n_check_features_sae\n",
    "del relevant_sae_activations\n",
    "t.cuda.empty_cache()\n",
    "all_relevant_sae_activations.shape, is_this_a_dot.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_relevant_sae_activations.shape\n",
    "is_this_a_dot.shape\n",
    "is_this_a_space = (encoded_inputs_tensor == meta['stoi'][' ']).to(device)\n",
    "is_this_a_space = is_this_a_space.long()\n",
    "is_this_a_space = is_this_a_space[:rows_to_include]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate_ones(row):\n",
    "    # Find indices where there are 1s\n",
    "    ones_indices = (row == 1).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Assign alternating 1 and 2\n",
    "    for i, idx in enumerate(ones_indices):\n",
    "        row[idx] = 1 if i % 2 == 0 else 2\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "result_tensor = torch.stack([alternate_ones(row.clone()) for row in is_this_a_space])\n",
    "is_this_a_first_space = result_tensor==1\n",
    "is_this_a_second_space = result_tensor==2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_on_dot = all_relevant_sae_activations * is_this_a_dot.unsqueeze(-1)\n",
    "activations_on_first_space = all_relevant_sae_activations * is_this_a_first_space.unsqueeze(-1)\n",
    "activations_on_second_space = all_relevant_sae_activations * is_this_a_second_space.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_activations = all_relevant_sae_activations.max(dim=0).values.max(dim=0).values\n",
    "del all_relevant_sae_activations\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacty of 23.69 GiB of which 1.77 GiB is free. Process 1810083 has 4.29 GiB memory in use. Process 1966692 has 3.36 GiB memory in use. Process 2038806 has 3.45 GiB memory in use. Process 2067979 has 5.27 GiB memory in use. Process 2091698 has 5.53 GiB memory in use. Of the allocated memory 5.19 GiB is allocated by PyTorch, and 39.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m threshold_multiplier \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43m(\u001b[49m\u001b[43mactivations_on_dot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold_multiplier\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_activations\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacty of 23.69 GiB of which 1.77 GiB is free. Process 1810083 has 4.29 GiB memory in use. Process 1966692 has 3.36 GiB memory in use. Process 2038806 has 3.45 GiB memory in use. Process 2067979 has 5.27 GiB memory in use. Process 2091698 has 5.53 GiB memory in use. Of the allocated memory 5.19 GiB is allocated by PyTorch, and 39.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "threshold_multiplier = 0.2\n",
    "(activations_on_dot > threshold_multiplier * max_activations).sum(dim=0).sum(dim=0).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
